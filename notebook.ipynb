{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carla speed limit assist with corrective actions and traffic light detection and warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import library\n",
    "- **cv2 (OpenCV):** computer vision library for image processing. Used for handling and processing video frames or images.\n",
    "- **paho.mqtt.client:** client library for the MQTT protocol, this library permit to publish MQTT topics, likely for exchanging data between the CARLA simulator and other systems.\n",
    "- **ultralytics (YOLO):** provides Python interface for the YOLO (You Only Look Once) object detection framework. It provide object detection tasks, such as identify traffic light, speedLimit.\n",
    "- **pygame:** library for creating games and multimedia applications, it allows to take input events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import paho.mqtt.client as mqtt\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "import pygame\n",
    "\n",
    "try:\n",
    "    sys.path.append(glob.glob('../../carla/dist/carla-*%d.%d-%s.egg' % (\n",
    "        sys.version_info.major,\n",
    "        sys.version_info.minor,\n",
    "        'win-amd64' if os.name == 'nt' else 'linux-x86_64'))[0])\n",
    "except IndexError:\n",
    "    pass\n",
    "\n",
    "import carla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broker MQTT configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\anaconda3\\envs\\carla-env\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Callback API version 1 is deprecated, update to latest version\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "BROKER = \"localhost\"\n",
    "PORT = 1883    \n",
    "\n",
    "clientMQTT = mqtt.Client()\n",
    "clientMQTT.connect(BROKER, PORT, 60)\n",
    "def sendEventToBroker(topic, message):\n",
    "    try:\n",
    "        clientMQTT.publish(topic, message)\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = carla.Client('localhost', 2000)\n",
    "client.set_timeout(10.0)\n",
    "client.load_world('Town02')\n",
    "world = client.get_world()\n",
    "spectator = world.get_spectator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful functions used in this notebook\n",
    "- **spawn_vehicle:** Adds a vehicle to the simulation.\n",
    "  - Picks a vehicle type from the blueprint library (pattern).\n",
    "  - Selects a spawn point on the map (spawn_index).\n",
    "  - Places the vehicle at that location.\n",
    "- **spawn_camera** Adds a camera to the simulation.\n",
    "  - Sets the camera size, field of view (foV), and update speed.\n",
    "  - Places the camera at a specific position and angle.\n",
    "  - Attaches the camera to a vehicle.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 640\n",
    "currect_map = False\n",
    "\n",
    "def spawn_vehicle(vehicle_index=0, spawn_index=0, pattern='vehicle.*'):\n",
    "    blueprint_library = world.get_blueprint_library()\n",
    "    vehicle_bp = blueprint_library.filter(pattern)[vehicle_index]\n",
    "    spawn_point = world.get_map().get_spawn_points()[spawn_index]\n",
    "    vehicle = world.spawn_actor(vehicle_bp, spawn_point)\n",
    "    return vehicle\n",
    "\n",
    "if world.get_map().name == \"Town02\" or world.get_map().name == \"Town01\":\n",
    "    currect_map = True\n",
    "\n",
    "def spawn_camera(attach_to=None, transform=carla.Transform(carla.Location(x=0.7, z=1.8), carla.Rotation(pitch=5, yaw=35)), width=IMAGE_SIZE, height=IMAGE_SIZE, foV=50):\n",
    "    camera_bp = world.get_blueprint_library().find('sensor.camera.rgb')\n",
    "    camera_bp.set_attribute('image_size_x', str(width))\n",
    "    camera_bp.set_attribute('image_size_y', str(height))\n",
    "    camera_bp.set_attribute('fov', str(foV))\n",
    "    camera_bp.set_attribute('sensor_tick', '0')\n",
    "    camera = world.spawn_actor(camera_bp, transform, attach_to=attach_to)\n",
    "    return camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training YOLOv8 Models for Object Detection\n",
    "\n",
    "### Overview\n",
    "We have trained the **YOLOv8** model multiple times to detect specific objects with two categories:\n",
    "\n",
    "1. **Speed Limits**:\n",
    "    - **First Training**: Using images of the speed limit signs without any background.\n",
    "    - **Second Training**: Using images of speed limit signs in real-world environments, with significant background clutter for object detection.\n",
    "\n",
    "2. **Traffic Lights**:\n",
    "    - **First Training**: Using images of the traffic lights without any background.\n",
    "    - **Second Training**: Using images of traffic lights in real-world environments, with significant background clutter for object detection.\n",
    "\n",
    "At the end, we will have **two YOLOv8 models**:\n",
    "- One dedicated to **speed limits**.\n",
    "- One dedicated to **traffic lights**.\n",
    "\n",
    "### Dataset\n",
    "The datasets used for training were sourced from the following site:  \n",
    "[Roboflow Universe](https://universe.roboflow.com/)\n",
    "\n",
    "### Base Code for Downloading and Training the Model\n",
    "Below is the base code to download the dataset and train the YOLOv8 model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install roboflow\n",
    "# from roboflow import Roboflow\n",
    "# rf = Roboflow(api_key=\"...\")\n",
    "# project = rf.workspace(\"wawan-pradana\").project(\"cinta_v2\")\n",
    "# dataset = project.version(1).download(\"yolov8\")\n",
    "# model = YOLO('yolov8n.pt')\n",
    "# results = model.train(data=\"data.yaml\", epochs=80, imgsz=416, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model Results\n",
    "<img src=\"runsSpeed/detect/train/confusion_matrix.png\" alt=\"Descrizione dell'immagine\" width=\"700\">\n",
    "<img src=\"runsTrafficLight/detect/train3/confusion_matrix.png\" alt=\"Descrizione dell'immagine\" width=\"700\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"runsSpeed/detect/train/weights/best.pt\")\n",
    "modelTrafficLight = YOLO(\"runsTrafficLight/detect/train3/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set weather conditions\n",
    "- Set the worst weather condition or the best weather condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = world.get_weather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.sun_azimuth_angle = 0\n",
    "weather.sun_altitude_angle = -90\n",
    "weather.cloudiness = 100\n",
    "weather.precipitation = 100\n",
    "weather.precipitation_deposits = 100\n",
    "weather.wind_intensity = 100\n",
    "weather.fog_density = 100\n",
    "weather.fog_distance = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.sun_azimuth_angle = 0 \n",
    "weather.sun_altitude_angle = 90\n",
    "weather.cloudiness = 20\n",
    "weather.precipitation = 0\n",
    "weather.precipitation_deposits = 0\n",
    "weather.wind_intensity = 0\n",
    "weather.fog_density = 30\n",
    "weather.fog_distance = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "world.set_weather(weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "- **colorSpeedLimit**: Change the letters color in the view.\n",
    "- **calcColor**: Change the letters color of the traffic light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorSpeedLimit(current_speed, speed_limit):\n",
    "    if speed_limit == None or int(current_speed) <= int(speed_limit):\n",
    "        return (0, 255, 0) \n",
    "    else:\n",
    "        return (0, 0, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcColor():\n",
    "    if traffic_light == \"red\":\n",
    "        return (0, 0, 255)\n",
    "    elif traffic_light == \"yellow\":\n",
    "        return (0, 255, 255)\n",
    "    elif traffic_light == \"green\":\n",
    "        return (0, 255, 0)\n",
    "    else:\n",
    "        return (255, 255, 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera_callback:\n",
    "\n",
    "This function processes an image captured by a camera, analyzes it using a machine learning model, and identifies the most likely speed limit sign if found with sufficient confidence:\n",
    "\n",
    "1. **Global Variables:**\n",
    "   - `last_analysis_time`: Tracks the time of the last analysis to ensure periodic processing.\n",
    "   - `last_speed_limit`: Stores the most recently detected speed limit.\n",
    "\n",
    "2. **Image Data Conversion:**\n",
    "   - The raw image data is read into a NumPy array using `np.frombuffer`, which converts the data into a byte array.\n",
    "   - The image is reshaped into its original dimensions, including the alpha channel (RGBA format).\n",
    "   - The alpha channel is removed to work with the RGB image, creating `image_bgr`.\n",
    "\n",
    "3. **Periodic Analysis:**\n",
    "   - Checks if enough time `analysis_interval` has passed since the last analysis using the current timestamp.\n",
    "   - If the time interval condition is met, updates `last_analysis_time`.\n",
    "\n",
    "4. **Model Prediction:**\n",
    "   - The resized image is passed to a machine learning model (`model`) for analysis.\n",
    "   - Extracts the detected class IDs (`class_ids`) and their confidence scores (`confidences`) from the model's results.\n",
    "\n",
    "5. **Identify Speed Limit:**\n",
    "   - Iterates through the detected objects, finding the class ID with the highest confidence score.\n",
    "   - If the confidence score exceeds a threshold (`0.87`), determines the corresponding class name.\n",
    "\n",
    "6. **Extract Speed Limit and Notify:**\n",
    "   - Attempts to extract the speed limit value from the class name.\n",
    "   - Updates `last_speed_limit` with the detected value.\n",
    "   - Sends an event with the detected speed limit to an external broker using `sendEventToBroker`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera_callback(image):\n",
    "    global last_analysis_time, last_speed_limit\n",
    "    current_time = time.time()\n",
    "\n",
    "    array = np.frombuffer(image.raw_data, dtype=np.uint8)\n",
    "    image_np = array.reshape((IMAGE_SIZE, IMAGE_SIZE, 4))\n",
    "    image_bgr = image_np[:, :, :3]  \n",
    "\n",
    "    if current_time - last_analysis_time >= analysis_interval:\n",
    "        last_analysis_time = current_time\n",
    "        results = model(image_bgr, verbose=False)\n",
    "        limit_id = None\n",
    "        confidence_max = 0\n",
    "        class_ids = results[0].boxes.cls.numpy()\n",
    "        confidences = results[0].boxes.conf.numpy()\n",
    "        \n",
    "        for class_id, confidence in zip(class_ids, confidences):\n",
    "            if confidence > confidence_max:\n",
    "                limit_id = class_id\n",
    "                confidence_max = confidence\n",
    "\n",
    "        if confidence_max > 0.87:\n",
    "            print(confidence_max)\n",
    "            class_name = class_names[int(limit_id)]\n",
    "            try:\n",
    "                last_speed_limit = class_name.split(\" \")[2]\n",
    "                sendEventToBroker(\"speedLimit\", \"Detected \" + last_speed_limit)\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera_view_callback:\n",
    "\n",
    "This function processes an image captured by a camera, calculates the vehicle's speed, and updates the global variables used for video output and speed monitoring:\n",
    "\n",
    "1. **Global Variables:**\n",
    "   - `video_output`: A global variable used to store the processed image data for visualization or further use.\n",
    "   - `speed_car`: A global variable used to store the vehicle's current speed.\n",
    "\n",
    "2. **Speed Calculation:**\n",
    "   - Retrieves the vehicle's velocity using `vehicle.get_velocity()`, which provides the velocity components along the X, Y, and Z axes.\n",
    "   - Computes the magnitude of the velocity vector using the formula for Euclidean norm.\n",
    "   - Converts the speed from meters per second (m/s) to kilometers per hour (km/h) by multiplying by 3.6.\n",
    "   - Updates the global variable `speed_car` with the calculated speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera_view_callback(image):\n",
    "    global video_output, speed_car\n",
    "    array = np.frombuffer(image.raw_data, dtype=np.uint8)\n",
    "    image_np = array.reshape((IMAGE_SIZE, IMAGE_SIZE, 4))\n",
    "    velocity_car = vehicle.get_velocity()\n",
    "    speed_car = 3.6 * (velocity_car.x**2 + velocity_car.y**2 + velocity_car.z**2)**0.5\n",
    "    video_output = image_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera_traffic_callback:\n",
    "This function processes an image from a camera to detect the current state of a traffic light, applying enhancements to the image and analyzing it with a machine learning model. It updates global variables and notifies changes:\n",
    "\n",
    "1. **Global Variables:**\n",
    "   - `last_analysis_time_trafficLight`: Tracks the time of the last traffic light analysis to ensure periodic processing.\n",
    "   - `traffic_light`: Stores the detected traffic light state (`\"red\"`, `\"yellow\"`, `\"green\"`, or `\"Not detected\"`).\n",
    "\n",
    "2. **Image Data Conversion:**\n",
    "   - Converts the raw image data from the camera into a NumPy array using `np.frombuffer`.\n",
    "   - Reshapes the array into an image matrix of size `IMAGE_SIZE x IMAGE_SIZE` with 4 channels (BGRA format).\n",
    "   - Creates a copy of the RGB portion (`image_bgr`) for further processing.\n",
    "\n",
    "3. **Image Preprocessing:**\n",
    "   - **Color Enhancement:** Increases the red channel values to accentuate red tones, reduces blue and green values slightly to suppress their dominance, and creates an enhanced image for better detection.\n",
    "   - **Clipping Values:** Ensures pixel values remain in the valid range (0–255).\n",
    "\n",
    "1. **Periodic Analysis:**\n",
    "   - Checks if the interval since the last analysis exceeds a defined threshold (`analysis_interval_trafficLight`).\n",
    "   - Updates `last_analysis_time_trafficLight` when a new analysis is performed.\n",
    "\n",
    "2. **Traffic Light Detection:**\n",
    "   - Passes the processed image (`resized_image`) to a traffic light detection model (`modelTrafficLight`).\n",
    "   - Retrieves the detected class IDs (`class_ids`) and their confidence scores (`confidences`).\n",
    "   - Handles cases where no traffic lights are detected by setting `traffic_light` to `\"Not detected\"`.\n",
    "\n",
    "3. **Identify Most Likely Traffic Light:**\n",
    "   - Iterates through detected objects, identifying the class ID with the highest confidence score.\n",
    "   - It assigns the detected class name as the current traffic light state.\n",
    "   - Applies confidence thresholds:\n",
    "     - High confidence (`>0.63`) for red light.\n",
    "     - Lower confidence (`>0.47`) for other colors.\n",
    "\n",
    "4. **Update and Notify:** Sends an event to an external broker using `sendEventToBroker` with the detected traffic light state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorMapper(accentuated_image):\n",
    "    accentuated_image[:, :, 2] = cv2.add(accentuated_image[:, :, 2], 16)\n",
    "    accentuated_image[:, :, 1] = cv2.subtract(accentuated_image[:, :, 1], 0)\n",
    "    accentuated_image[:, :, 0] = cv2.subtract(accentuated_image[:, :, 0], 20)\n",
    "    return np.clip(accentuated_image, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera_traffic_callback(image):\n",
    "    global last_analysis_time_trafficLight, traffic_light, video_output2\n",
    "    current_time = time.time()\n",
    "    array = np.frombuffer(image.raw_data, dtype=np.uint8)\n",
    "    image_np = array.reshape((IMAGE_SIZE, IMAGE_SIZE, 4))\n",
    "    image_bgr = image_np[:, :, :3].copy()\n",
    "    resized_image = colorMapper(image_bgr.copy())\n",
    "    resized_image[:200, :] = 0\n",
    "    video_output2 = resized_image\n",
    "\n",
    "    if current_time - last_analysis_time_trafficLight >= analysis_interval_trafficLight:\n",
    "        last_analysis_time_trafficLight = current_time\n",
    "        results = modelTrafficLight(resized_image, verbose=False)\n",
    "        trafficLight_id = None\n",
    "        confidence_max = 0\n",
    "        class_ids = results[0].boxes.cls.numpy()\n",
    "        confidences = results[0].boxes.conf.numpy()\n",
    "\n",
    "        if class_ids.size == 0:\n",
    "            traffic_light = \"Not detected\"\n",
    "        else:\n",
    "            for class_id, confidence in zip(class_ids, confidences):\n",
    "                if confidence > confidence_max:\n",
    "                    trafficLight_id = class_id\n",
    "                    confidence_max = confidence\n",
    "\n",
    "            if traffic_light != None and class_names_trafficLight[int(trafficLight_id)] == \"yellow\" and traffic_light == \"red\":\n",
    "                class_name = \"red\"\n",
    "            else:\n",
    "                class_name = class_names_trafficLight[int(trafficLight_id)]\n",
    "            if confidence_max > 0.63 and class_name==\"red\" or confidence_max > 0.47 and class_name!=\"red\":\n",
    "                print(confidence_max)\n",
    "                if traffic_light != class_name:\n",
    "                    traffic_light = class_name\n",
    "                    sendEventToBroker(\"TrafficLight\", \"Detected trafficlight \" + class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera_traffic_red_callback:\n",
    "\n",
    "This function operates similarly to `camera_traffic_callback(image)` but focuses specifically on detecting and reacting to **red traffic lights**, modifying a global flag (`red_over`) based on the detection results:\n",
    "\n",
    "1. **Global Variables:**\n",
    "   - `last_analysis_time_trafficLight_red`: Tracks the time of the last analysis for red traffic lights to ensure periodic updates.\n",
    "   - `red_over`: A flag indicating whether a red light condition has been detected and action of stopping should be enforced.\n",
    "\n",
    "2. **Traffic Light Detection:**\n",
    "   - Passes the processed image to the `modelTrafficLight`, which detects traffic light states and outputs class IDs (`class_ids`) and their confidence scores (`confidences`).\n",
    "   - Handles cases where no traffic lights are detected by resetting `red_over` to `False`.\n",
    "\n",
    "3. **Red Light Detection Logic:**\n",
    "   - Iterates through detected objects, identifying the class ID with the highest confidence score.\n",
    "   - If the detected class is `\"red\"` and the confidence score exceeds `0.65`: Sets `red_over` to `True` **unless the vehicle is in reverse (`gear != \"Reverse\"`)** or another condition prevents it.\n",
    "   - If the detected class is `\"green\"` or `\"yellow\"` and the confidence score exceeds `0.43`: Resets `red_over` to `False`, allowing movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera_traffic_red_callback(image):\n",
    "    global last_analysis_time_trafficLight_red, red_over\n",
    "    current_time = time.time()\n",
    "    array = np.frombuffer(image.raw_data, dtype=np.uint8)\n",
    "    image_np = array.reshape((IMAGE_SIZE, IMAGE_SIZE, 4))\n",
    "    image_bgr = image_np[:, :, :3].copy()\n",
    "    resized_image = colorMapper(image_bgr.copy())\n",
    "\n",
    "    if current_time - last_analysis_time_trafficLight_red >= analysis_interval_trafficLight_red:\n",
    "        last_analysis_time_trafficLight_red = current_time\n",
    "        results = modelTrafficLight(resized_image, verbose=False)\n",
    "\n",
    "        trafficLight_id = None\n",
    "        confidence_max = 0\n",
    "\n",
    "        class_ids = results[0].boxes.cls.numpy()\n",
    "        confidences = results[0].boxes.conf.numpy()\n",
    "\n",
    "        if class_ids.size != 0:\n",
    "            for class_id, confidence in zip(class_ids, confidences):\n",
    "                if confidence > confidence_max:\n",
    "                    trafficLight_id = class_id\n",
    "                    confidence_max = confidence\n",
    "\n",
    "            class_name = class_names_trafficLight[int(trafficLight_id)]\n",
    "            if class_name==\"red\" and confidence_max > 0.65:\n",
    "                if(gear != \"Reverse\" and not red_over):\n",
    "                    red_over = True\n",
    "            elif (class_name==\"green\" or class_name==\"yellow\") and confidence_max > 0.43:\n",
    "                red_over = False\n",
    "        else:\n",
    "            red_over = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of the Code\n",
    "\n",
    "This code use camera feeds, traffic light detection, and manual/automatic controls:\n",
    "\n",
    "### 1. **Setup and Initialization**\n",
    "   - **Pygame Window:** Creates a minimal interface using Pygame, used for event handling keyboard inputs.\n",
    "   - **Global Variables:**\n",
    "     - Includes control parameters for throttle, brake, and steering.\n",
    "     - Flags for managing speed control, red light detection, and traffic light states.\n",
    "     - Constants for proportional control of throttle and brake adjustments based on speed error.\n",
    "   - **Timers and Analysis Intervals:** Tracks the timing of camera analyses for speed, traffic lights, and red light violations.\n",
    "   - **Camera Feeds:** Initializes multiple cameras:\n",
    "     - Main RGB camera for capturing the environment.\n",
    "     - Additional cameras for detecting traffic lights, speed limits and specific red light conditions.\n",
    "   - **Vehicle Spawning:** Spawns a simulated vehicle in CARLA and attaches the cameras to it.\n",
    "\n",
    "### 2. **Real-Time Display and Feedback**\n",
    "   - Displays:\n",
    "     - Current speed and detected speed limits.\n",
    "     - Traffic light state (e.g., `\"red\"`, `\"green\"`).\n",
    "     - A warning if the vehicle violates a red light.\n",
    "\n",
    "### 3. **Camera-Based Analysis**\n",
    "   - Each camera captures frames periodically, which are processed for specific tasks:\n",
    "     - **Speed Limit Detection:** Uses object detection to identify speed limit signs.\n",
    "     - **Traffic Light State Detection:** Identifies traffic light states and updates global flags.\n",
    "     - **Red Light Violations:** Specifically monitors red lights and determines if the vehicle violates them.\n",
    "\n",
    "### 4. **Event Handling and Controls**\n",
    "   - **Keyboard Controls:**\n",
    "     - Toggle speed control (`E`), red light detection (`O`), and autopilot (enable `C`, disable `V`).\n",
    "     - Manual acceleration: (`W`), braking (`S`), steering (`A`/`D`), and gear selection (`R`/`F`).\n",
    "   - **Speed Control Logic:**\n",
    "     - Automatically adjusts throttle and brake based on the speed limit.\n",
    "     - Includes a \"dead zone\" around the speed limit to prevent constant adjustments.\n",
    "   - **Red Light Response:**\n",
    "     - Automatically engages brakes when a red light is detected and speed under 40 km/h.\n",
    "\n",
    "### 5. **Vehicle State Updates**\n",
    "   - Continuously updates the vehicle's control inputs (`throttle`, `brake`, `steer`) based on manual or automatic inputs.\n",
    "   - Ensures smooth transitions, such as gradual steering return to center when no input is applied.\n",
    "\n",
    "### 6. **Cleanup**\n",
    "   - Gracefully terminates the simulation, destroying the cameras, vehicle, and display, and disconnecting any external connections (e.g., MQTT broker).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "bn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34352\\1667863558.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mfoV\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     )\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[0mcamera_trafficLight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlisten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcamera_traffic_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     camera_trafficLight_red_detector = spawn_camera(\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34352\\146142839.py\u001b[0m in \u001b[0;36mcamera_traffic_callback\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcurrent_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlast_analysis_time_trafficLight\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0manalysis_interval_trafficLight\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mlast_analysis_time_trafficLight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodelTrafficLight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresized_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mtrafficLight_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mconfidence_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\carla-env\\lib\\site-packages\\ultralytics\\engine\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;34m\"\"\"Calls the 'predict' function with given arguments to perform object detection.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\carla-env\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\carla-env\\lib\\site-packages\\ultralytics\\engine\\model.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[0mpredictor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictor\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmart_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predictor'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_callbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_cli\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# only update args if predictor is already setup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_cfg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\carla-env\\lib\\site-packages\\ultralytics\\engine\\predictor.py\u001b[0m in \u001b[0;36msetup_model\u001b[1;34m(self, model, verbose)\u001b[0m\n\u001b[0;32m    309\u001b[0m                                  \u001b[0mfp16\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhalf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                                  \u001b[0mfuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m                                  verbose=verbose)\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m  \u001b[1;31m# update device\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\carla-env\\lib\\site-packages\\ultralytics\\nn\\autobackend.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, weights, device, dnn, data, fp16, fuse, verbose)\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnn_module\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# in-memory PyTorch model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfuse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mfuse\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'kpt_shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mkpt_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkpt_shape\u001b[0m  \u001b[1;31m# pose-only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\carla-env\\lib\\site-packages\\ultralytics\\nn\\tasks.py\u001b[0m in \u001b[0;36mfuse\u001b[1;34m(self, verbose)\u001b[0m\n\u001b[0;32m    132\u001b[0m                         \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfuse_convs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                     \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfuse_conv_and_bn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# update conv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                     \u001b[0mdelattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'bn'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# remove batchnorm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m                     \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_fuse\u001b[0m  \u001b[1;31m# update forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConvTranspose\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'bn'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\andre\\anaconda3\\envs\\carla-env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__delattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1326\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1328\u001b[1;33m             \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__delattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_register_state_dict_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: bn"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.145  Python-3.7.12 torch-1.13.1+cpu CPU (12th Gen Intel Core(TM) i7-12700H)\n",
      "Ultralytics YOLOv8.0.145  Python-3.7.12 torch-1.13.1+cpu CPU (12th Gen Intel Core(TM) i7-12700H)\n",
      "Model summary (fused): 168 layers, 3006233 parameters, 0 gradients\n",
      "Model summary (fused): 168 layers, 3006233 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55957806\n",
      "0.47419462\n",
      "0.7418785\n",
      "0.70253676\n",
      "0.73391205\n",
      "0.6868359\n",
      "0.7468788\n",
      "0.74011266\n",
      "0.80277795\n",
      "0.8036567\n",
      "0.82382774\n",
      "0.83640695\n",
      "0.8711809\n",
      "0.87782943\n",
      "0.9160037\n",
      "0.73094726\n",
      "0.8176154\n",
      "0.86154443\n",
      "0.98370636\n",
      "0.9318965\n",
      "0.8838002\n",
      "0.922107\n",
      "0.89763284\n",
      "0.91464865\n",
      "0.6798841\n",
      "0.81477666\n",
      "0.87558\n",
      "0.65239424\n",
      "0.78243285\n",
      "0.8714172\n",
      "0.97862405\n",
      "0.8977685\n",
      "0.96722823\n",
      "0.965674\n",
      "0.8832099\n",
      "0.9509244\n",
      "0.9757657\n",
      "0.91567034\n",
      "0.9247061\n",
      "0.8027396\n",
      "0.89462006\n",
      "0.6993734\n",
      "0.737652\n",
      "0.92872083\n",
      "0.93531173\n",
      "0.9142315\n",
      "0.92993563\n",
      "0.9302993\n",
      "0.62669486\n",
      "0.7233489\n",
      "0.85869837\n",
      "0.8460285\n",
      "0.67573\n",
      "0.80541176\n",
      "0.8581924\n",
      "0.96815187\n",
      "0.9765445\n",
      "0.91811186\n",
      "0.9215567\n",
      "0.93695045\n",
      "0.9747138\n",
      "0.9277649\n",
      "0.93585455\n"
     ]
    }
   ],
   "source": [
    "# Initialize a small Pygame display for handling events\n",
    "display = pygame.display.set_mode((50, 50))\n",
    "pygame.display.set_caption(\"Manual Control CARLA\")\n",
    "\n",
    "# Define control variables and increments\n",
    "steer_increment = 0.02  # Increment for steering adjustments\n",
    "throttle_increment = 0.1  # Increment for throttle adjustments\n",
    "steer = 0.0  # Current steering value\n",
    "throttle = 0.0  # Current throttle value\n",
    "brake = 0.0  # Current brake value\n",
    "global gear  # Define a global gear variable\n",
    "gear = \"Drive\"  # Initial gear state\n",
    "speed_control_activate = False  # Flag for speed control activation\n",
    "red_over_activate = False  # Flag for red light detection activation\n",
    "red_over = False  # Red light violation flag\n",
    "\n",
    "# Constants for proportional controller\n",
    "KP_THROTTLE = 0.15  # Proportional gain for acceleration\n",
    "KP_BRAKE = 0.02  # Proportional gain for braking\n",
    "DEAD_ZONE = 3.0  # Dead zone around the speed limit\n",
    "MIN_THROTTLE = 0.2  # Minimum throttle value\n",
    "MIN_BRAKE = 0.1  # Minimum brake value\n",
    "\n",
    "# Timing variables for periodic analyses\n",
    "last_analysis_time = 0  # Last analysis timestamp for speed limit detection\n",
    "last_analysis_time_trafficLight = 0  # Last analysis timestamp for traffic light\n",
    "last_analysis_time_trafficLight_red = 0  # Last analysis timestamp for red light detection\n",
    "analysis_interval = 0.2  # Time interval for speed analysis\n",
    "analysis_interval_trafficLight = 0.3  # Time interval for traffic light analysis\n",
    "analysis_interval_trafficLight_red = 0.3  # Time interval for red light analysis\n",
    "\n",
    "# Initialize the output video frame\n",
    "video_output = np.zeros((IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.uint8)\n",
    "\n",
    "# Variables for speed and traffic light tracking\n",
    "last_speed_limit = None  # Last detected speed limit\n",
    "speed_car = 0  # Current vehicle speed\n",
    "traffic_light = \"Not detected\"  # Current traffic light state\n",
    "\n",
    "# Load class names for object detection models\n",
    "class_names = model.names  # Object detection model names\n",
    "class_names_trafficLight = modelTrafficLight.names  # Traffic light model names\n",
    "\n",
    "# Spawn the simulated vehicle\n",
    "vehicle = spawn_vehicle()\n",
    "\n",
    "# Attach and configure cameras for various purposes\n",
    "camera = spawn_camera(attach_to=vehicle)  # Main camera\n",
    "camera.listen(lambda image: camera_callback(image))  # Listen for incoming frames\n",
    "\n",
    "camera_view = spawn_camera(\n",
    "    attach_to=vehicle, \n",
    "    transform=carla.Transform(carla.Location(x=1, z=1.5), carla.Rotation(pitch=0, yaw=0)), \n",
    "    width=IMAGE_SIZE, \n",
    "    height=IMAGE_SIZE, \n",
    "    foV=80\n",
    ")  # Driver's perspective camera\n",
    "camera_view.listen(lambda image: camera_view_callback(image))\n",
    "\n",
    "# Additional cameras for traffic light and red light detection\n",
    "if currect_map:\n",
    "    camera_trafficLight = spawn_camera(\n",
    "        attach_to=vehicle, \n",
    "        transform=carla.Transform(carla.Location(x=0, y=1, z=1.2), carla.Rotation(pitch=20, yaw=10)), \n",
    "        width=IMAGE_SIZE, \n",
    "        height=IMAGE_SIZE, \n",
    "        foV=40\n",
    "    )\n",
    "    camera_trafficLight.listen(lambda image: camera_traffic_callback(image))\n",
    "    \n",
    "    camera_trafficLight_red_detector = spawn_camera(\n",
    "        attach_to=vehicle, \n",
    "        transform=carla.Transform(carla.Location(x=-1.8, y=0, z=2), carla.Rotation(pitch=0, yaw=0)), \n",
    "        width=IMAGE_SIZE, \n",
    "        height=IMAGE_SIZE, \n",
    "        foV=110\n",
    "    )\n",
    "    camera_trafficLight_red_detector.listen(lambda image: camera_traffic_red_callback(image))\n",
    "\n",
    "# Create an OpenCV window for displaying results\n",
    "cv2.namedWindow('RGB Camera', cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "cv2.namedWindow('RGB Camera2', cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "try:\n",
    "    clock = pygame.time.Clock()  # Pygame clock for controlling loop rate\n",
    "    event_timer = 0  # Timer for event processing\n",
    "    EVENT_RATE = 100  # Event handling interval in milliseconds\n",
    "\n",
    "    while True:\n",
    "        # Add text overlays to the output frame\n",
    "        temp_frame = video_output.copy()\n",
    "        cv2.putText(\n",
    "            temp_frame,\n",
    "            f\"Last Speed Limit: {last_speed_limit}\",\n",
    "            (10, 20),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            (0, 255, 0),\n",
    "            2,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "        cv2.putText(\n",
    "            temp_frame,\n",
    "            f\"Current speed: {speed_car:.0f}\",\n",
    "            (10, 45),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            colorSpeedLimit(speed_car, last_speed_limit),\n",
    "            2,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "        # Display traffic light information and warnings\n",
    "        if currect_map:\n",
    "            if red_over:\n",
    "                val = \"You ran a red light\"\n",
    "                sendEventToBroker(\"TrafficLightViolation\", \"Red light violation\")\n",
    "            else:\n",
    "                val = \"\"\n",
    "            cv2.putText(\n",
    "                temp_frame,\n",
    "                val,\n",
    "                (10, 100),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.5,\n",
    "                (0, 0, 255),\n",
    "                2,\n",
    "                cv2.LINE_AA,\n",
    "            )\n",
    "        cv2.putText(\n",
    "            temp_frame,\n",
    "            f\"Traffic light: {traffic_light}\",\n",
    "            (10, 70),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            calcColor(),\n",
    "            2,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "        # Show the frame with overlays\n",
    "        cv2.imshow('RGB Camera', temp_frame)\n",
    "        cv2.imshow('RGB Camera2', video_output2.copy())\n",
    "\n",
    "        # Break the loop on 'q' key press\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "        # Handle Pygame events periodically\n",
    "        current_time = pygame.time.get_ticks()\n",
    "        if current_time - event_timer > EVENT_RATE:\n",
    "            event_timer = current_time\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    break\n",
    "                elif event.type == pygame.KEYUP:  # Handle key releases\n",
    "                    if event.key == pygame.K_e:  # Toggle speed control\n",
    "                        speed_control_activate = not speed_control_activate\n",
    "                    if event.key == pygame.K_c:  # Enable autopilot\n",
    "                        vehicle.set_autopilot(True)\n",
    "                    if event.key == pygame.K_v:  # Disable autopilot\n",
    "                        vehicle.set_autopilot(False)\n",
    "                    if event.key == pygame.K_o:  # Toggle red light detection\n",
    "                        red_over_activate = not red_over_activate\n",
    "\n",
    "            keys = pygame.key.get_pressed()  # Get pressed keys\n",
    "\n",
    "            # Handle speed control logic\n",
    "            if speed_control_activate and last_speed_limit and keys[pygame.K_w]:\n",
    "                # Calculate speed error relative to the limit\n",
    "                error = float(last_speed_limit) - float(speed_car) + 3\n",
    "                if error < -DEAD_ZONE:\n",
    "                    throttle = 0\n",
    "                    brake = max(KP_BRAKE * abs(error), MIN_BRAKE)\n",
    "                else:\n",
    "                    throttle = max(KP_THROTTLE * error, MIN_THROTTLE)\n",
    "                    brake = 0\n",
    "            elif keys[pygame.K_w]:  # Manual throttle control\n",
    "                throttle = min(throttle + throttle_increment, 1)\n",
    "                brake = 0\n",
    "            else:\n",
    "                throttle = 0\n",
    "                brake = 0\n",
    "            \n",
    "            if keys[pygame.K_s]:  # Manual brake control\n",
    "                brake = min(brake + throttle_increment * 4, 1)\n",
    "                throttle = 0\n",
    "\n",
    "            if red_over and int(speed_car) <= 40 and gear != \"Reverse\" and red_over_activate:\n",
    "                brake = 1\n",
    "                throttle = 0\n",
    "\n",
    "            # Handle steering controls\n",
    "            if keys[pygame.K_r]:  # Reverse gear\n",
    "                gear = \"Reverse\"\n",
    "                red_over = False\n",
    "            elif keys[pygame.K_f]:  # Forward gear\n",
    "                gear = \"Drive\"\n",
    "            elif keys[pygame.K_a]:  # Steer left\n",
    "                steer = max(steer - steer_increment, -1)\n",
    "            elif keys[pygame.K_d]:  # Steer right\n",
    "                steer = min(steer + steer_increment, 1)\n",
    "            else:\n",
    "                steer = steer * 0.9  # Gradually return to center\n",
    "\n",
    "            # Apply vehicle controls\n",
    "            control = carla.VehicleControl()\n",
    "            control.throttle = throttle\n",
    "            control.brake = brake\n",
    "            control.steer = steer\n",
    "            control.reverse = (gear == \"Reverse\")\n",
    "            vehicle.apply_control(control)\n",
    "\n",
    "        clock.tick(40)  # Maintain a 40 FPS loop rate\n",
    "finally:\n",
    "    # Clean up resources and connections\n",
    "    cv2.destroyAllWindows()\n",
    "    pygame.quit()\n",
    "    camera.destroy()\n",
    "    camera_trafficLight.destroy()\n",
    "    camera_trafficLight_red_detector.destroy()\n",
    "    camera_view.destroy()\n",
    "    vehicle.destroy()\n",
    "    clientMQTT.disconnect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carla-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
